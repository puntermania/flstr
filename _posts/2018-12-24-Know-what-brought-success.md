---
layout: post
title: Know what brought success
description: knowing what brought you success is more important than success itself
keywords: product management, product manager, success metrics, impact, product success, drive engagement
---

<p style="text-align: justify;line-height: 1.7"> 
Consistency in the results is critical to measure whether the product or the feature is a success. Very often I have seen a product manager celebrating the success prematurely and attributing that success to changes he/she did. I have been guilty of doing the same early in my career. </p> <br />

<p style="text-align: justify;line-height: 1.7">
After going through the typical exercise of finding the pain points of the users, prioritizing the pain points per their severity and criticality for the users, coming up with many solution ideas, and prioritizing those solutions that will help me achieve maximum possible impact in the shortest possible with least amount of resources, I piloted a feature all jazzed up to drive user engagement, especially time spent doing certain activities on the product. Boy! I drove the engagement or so I thought. Having released the feature, I started tracking metrics to assess the level of engagement. I was happy to see the metrics moving in the positive direction at a much faster rate than what I had initially thought. After reaching the upper bound, the growth slowed down and was almost getting plateaued, as unlike money, the growth doesn’t compound infinitely. To think of it, the engagement, as measured in terms of time spent on the platform, can be increased 5x but it’s hard to sustain that level of growth weekly, monthly or yearly consistently. </p> <br />

<p style="text-align: justify;line-height: 1.7">
Just as I started celebrating the impact and highlighting how the feature I built moved a key business metric in the positive direction faster, I noticed that engagement started dropping. The very key business metric that was moving up, was now sliding down. What I had observed was not consistent. The metrics that had grown and shown behaviors of growth in the last couple of weeks, were now rearing to move towards their original levels, without me having done any changes. I then dived deep to understand what is happening and why it’s happening. After coming up with lot of hypotheses, I found out that the high engagement mirrored a marketing campaign run by the company unbeknownst to me. When the plug was pulled on the marketing campaign, the engagement metrics tapered. </p> <br />

<p style="text-align: justify;line-height: 1.7">
It was a bitter pill to swallow. I prematurely celebrated the success. I did not vet and validate why there was a success. It’s a good trait to have, one which keeps a person grounded. I was quick to assume that what I built is the reason for the success. I did not consider that there could be many macro factors that moved those metrics.  Instead of the marketing campaign, it could have been that competition might have done something wrong which pushed users looking for alternatives on to my platform. Can I call it the success of my product? </p> <br />

<p style="text-align: justify;line-height: 1.7">
It could also be that changes have happened in distribution channels which has made it easier for the product to be found and it just so happens that it coincided with the feature release. It’s not common to see Google making changes that impacts the SEO one has worked for ages. It could also be that a major event in the market might have led to growth in the engagement. All of these demonstrate that key business metric(s) moved regardless of product changes. It’s thus important that one validates and vets what led to the success. </p> <br />

<p style="text-align: justify;line-height: 1.7">
A product manager thus must always validate that the features he/she has released are moving business metrics in the desired direction consistently. That’s when the feature is called a success. Not measuring consistently, incorrect attribution of successes and not identifying what led to those successes makes a product manager repeat the same mistakes in future and live in a bubble thinking that their strategy works. </p> <br />

<form action="https://gmail.us20.list-manage.com/subscribe/post?u=0e628327d496d7cbe86598540&amp;id=801bf936e2" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2>Subscribe to our mailing list</h2>
<div class="mc-field-group">
	<label for="mce-EMAIL">Email  <span class="asterisk">*</span>
</label>
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL">
</div>
<div class="mc-field-group">
	<label for="mce-FNAME">Name  <span class="asterisk">*</span>
</label>
	<input type="text" value="" name="FNAME" class="required" id="mce-FNAME">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_0e628327d496d7cbe86598540_801bf936e2" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>

<script type='text/javascript' src='//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js'></script><script type='text/javascript'>(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);</script>


